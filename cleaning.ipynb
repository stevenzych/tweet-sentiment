{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans and tokenizes Twitter data found [here](https://data.world/crowdflower/brands-and-product-emotions) for use in machine learning in the next notebook. It produces to separate datasets. One is lemmatized and one is stemmed, but the preceding cleaning and tokenization is identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"max_columns\", None)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import re\n",
    "\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8721, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "6                                                NaN   \n",
       "7  #SXSW is just starting, #CTIA is around the co...   \n",
       "8  Beautifully smart and simple idea RT @madebyma...   \n",
       "9  Counting down the days to #sxsw plus strong Ca...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "5                             NaN   \n",
       "6                             NaN   \n",
       "7                         Android   \n",
       "8              iPad or iPhone App   \n",
       "9                           Apple   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  \n",
       "5                 No emotion toward brand or product  \n",
       "6                 No emotion toward brand or product  \n",
       "7                                   Positive emotion  \n",
       "8                                   Positive emotion  \n",
       "9                                   Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/judge_1377884607_tweet_product_company.csv')\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         directed_at  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "            emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'tweet_text' : 'text',\n",
    "                   'is_there_an_emotion_directed_at_a_brand_or_product' : 'emotion',\n",
    "                   'emotion_in_tweet_is_directed_at' : 'directed_at'},\n",
    "          inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text              1\n",
       "directed_at    5552\n",
       "emotion           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = ['text'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text              0\n",
       "directed_at    5551\n",
       "emotion           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not replacing NaN's if emotion is undirected. Often it seems they actually *are* directed at a brand, but I don't have time to manually go through and label these. Plus, I'm only using that feature for plotting EDA for the presentation. It won't actually be fed into the NLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping `\"I can't tell\"` Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.emotion != \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following process creates a DataFrame of cleaned and tokenized tweets. Each tweet is replaced with a list of tokens. There are no user handles, hashtags, or web addresses. Punctuation and stopwords have also been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    text = re.sub('@\\S+', '', text)\n",
    "    text = re.sub('http\\S+', '', text)\n",
    "    text = re.sub('#\\S+', '', text)\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i, '').lower()\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.lower() not in stop_words:\n",
    "            new_tokens.append(token)\n",
    "            \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_clean)):\n",
    "    df_clean.iloc[i].text = basic_clean(df_clean.iloc[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[3g, iphone, 3, hrs, tweeting, dead, need, upg...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[know, awesome, ipadiphone, app, youll, likely...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[wait, 2, also, sale]</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[hope, years, festival, isnt, crashy, years, i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[great, stuff, fri, marissa, mayer, google, ti...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         directed_at  \\\n",
       "0  [3g, iphone, 3, hrs, tweeting, dead, need, upg...              iPhone   \n",
       "1  [know, awesome, ipadiphone, app, youll, likely...  iPad or iPhone App   \n",
       "2                              [wait, 2, also, sale]                iPad   \n",
       "3  [hope, years, festival, isnt, crashy, years, i...  iPad or iPhone App   \n",
       "4  [great, stuff, fri, marissa, mayer, google, ti...              Google   \n",
       "\n",
       "            emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma = df_clean.copy()\n",
    "df_stem = df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_lemma)):\n",
    "    for x in range(len(df_lemma.iloc[i].text)):\n",
    "        df_lemma.iloc[i].text[x] = lemmatizer.lemmatize(df_lemma.iloc[i].text[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[3g, iphone, 3, hr, tweeting, dead, need, upgr...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[know, awesome, ipadiphone, app, youll, likely...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[wait, 2, also, sale]</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[hope, year, festival, isnt, crashy, year, iph...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[great, stuff, fri, marissa, mayer, google, ti...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         directed_at  \\\n",
       "0  [3g, iphone, 3, hr, tweeting, dead, need, upgr...              iPhone   \n",
       "1  [know, awesome, ipadiphone, app, youll, likely...  iPad or iPhone App   \n",
       "2                              [wait, 2, also, sale]                iPad   \n",
       "3  [hope, year, festival, isnt, crashy, year, iph...  iPad or iPhone App   \n",
       "4  [great, stuff, fri, marissa, mayer, google, ti...              Google   \n",
       "\n",
       "            emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(language = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_stem)):\n",
    "    for x in range(len(df_stem.iloc[i].text)):\n",
    "        df_stem.iloc[i].text[x] = stemmer.stem(df_stem.iloc[i].text[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[3g, iphon, 3, hr, tweet, dead, need, upgrad, ...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[know, awesom, ipadiphon, app, youll, like, ap...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[wait, 2, also, sale]</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[hope, year, festiv, isnt, crashi, year, iphon...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[great, stuff, fri, marissa, mayer, googl, tim...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         directed_at  \\\n",
       "0  [3g, iphon, 3, hr, tweet, dead, need, upgrad, ...              iPhone   \n",
       "1  [know, awesom, ipadiphon, app, youll, like, ap...  iPad or iPhone App   \n",
       "2                              [wait, 2, also, sale]                iPad   \n",
       "3  [hope, year, festiv, isnt, crashi, year, iphon...  iPad or iPhone App   \n",
       "4  [great, stuff, fri, marissa, mayer, googl, tim...              Google   \n",
       "\n",
       "            emotion  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting CSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma.to_csv(\"data/df_lemma.csv\")\n",
    "df_stem.to_csv(\"data/df_stem.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More EDA and Stuff - Ignore For Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts(text):\n",
    "    wordcount = Counter()\n",
    "    for i in text.values:\n",
    "        for x in i:\n",
    "            wordcount[x] += 1\n",
    "    return wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to reassign this cause of weird errors above. No time to explore\n",
    "df_clean = df.copy()\n",
    "for i in range(len(df_clean)):\n",
    "    df_clean.iloc[i].text = basic_clean(df_clean.iloc[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting this CSV too\n",
    "df_clean.to_csv(\"data/df_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('link', 4093),\n",
       " ('rt', 2942),\n",
       " ('ipad', 2101),\n",
       " ('google', 1956),\n",
       " ('apple', 1684),\n",
       " ('store', 1398),\n",
       " ('iphone', 1208),\n",
       " ('new', 1063),\n",
       " ('2', 1040),\n",
       " ('austin', 815),\n",
       " ('app', 757),\n",
       " ('amp', 695),\n",
       " ('launch', 626),\n",
       " ('social', 602),\n",
       " ('popup', 556),\n",
       " ('today', 553),\n",
       " ('circles', 519),\n",
       " ('sxsw', 463),\n",
       " ('network', 450),\n",
       " ('android', 423),\n",
       " ('via', 406),\n",
       " ('line', 388),\n",
       " ('get', 383),\n",
       " ('called', 360),\n",
       " ('free', 353),\n",
       " ('party', 319),\n",
       " ('major', 302),\n",
       " ('mobile', 292),\n",
       " ('like', 279),\n",
       " ('one', 261),\n",
       " ('time', 259),\n",
       " ('temporary', 254),\n",
       " ('im', 249),\n",
       " ('���', 246),\n",
       " ('possibly', 244),\n",
       " ('opening', 242),\n",
       " ('people', 220),\n",
       " ('going', 216),\n",
       " ('see', 216),\n",
       " ('downtown', 214),\n",
       " ('check', 211),\n",
       " ('great', 210),\n",
       " ('day', 210),\n",
       " ('maps', 207),\n",
       " ('w', 203),\n",
       " ('apps', 200),\n",
       " ('go', 200),\n",
       " ('dont', 199),\n",
       " ('need', 197),\n",
       " ('mayer', 197),\n",
       " ('open', 189),\n",
       " ('marissa', 185),\n",
       " ('got', 181),\n",
       " ('know', 177),\n",
       " ('googles', 174),\n",
       " ('come', 172),\n",
       " ('first', 163),\n",
       " ('win', 162),\n",
       " ('good', 156),\n",
       " ('us', 156),\n",
       " ('ipad2', 152),\n",
       " ('pop', 148),\n",
       " ('next', 143),\n",
       " ('love', 142),\n",
       " ('cool', 140),\n",
       " ('want', 138),\n",
       " ('best', 134),\n",
       " ('panel', 134),\n",
       " ('design', 132),\n",
       " ('make', 132),\n",
       " ('think', 131),\n",
       " ('game', 130),\n",
       " ('thanks', 128),\n",
       " ('news', 127),\n",
       " ('shop', 126),\n",
       " ('big', 126),\n",
       " ('would', 124),\n",
       " ('around', 123),\n",
       " ('use', 123),\n",
       " ('search', 122),\n",
       " ('last', 122),\n",
       " ('awesome', 121),\n",
       " ('set', 119),\n",
       " ('4', 119),\n",
       " ('music', 118),\n",
       " ('users', 116),\n",
       " ('anyone', 115),\n",
       " ('talk', 115),\n",
       " ('show', 114),\n",
       " ('using', 113),\n",
       " ('says', 112),\n",
       " ('right', 111),\n",
       " ('video', 111),\n",
       " ('download', 108),\n",
       " ('rumor', 108),\n",
       " ('even', 108),\n",
       " ('guy', 107),\n",
       " ('still', 105),\n",
       " ('really', 105),\n",
       " ('session', 104),\n",
       " ('launching', 103),\n",
       " ('year', 102),\n",
       " ('coming', 100),\n",
       " ('1', 100),\n",
       " ('booth', 99),\n",
       " ('hey', 98),\n",
       " ('congress', 98),\n",
       " ('ipads', 97),\n",
       " ('location', 96),\n",
       " ('week', 95),\n",
       " ('team', 94),\n",
       " ('u', 94),\n",
       " ('heard', 94),\n",
       " ('products', 94),\n",
       " ('buy', 93),\n",
       " ('6th', 93),\n",
       " ('case', 92),\n",
       " ('future', 91),\n",
       " ('apples', 90),\n",
       " ('way', 89),\n",
       " ('ive', 87),\n",
       " ('well', 87),\n",
       " ('tonight', 86),\n",
       " ('twitter', 85),\n",
       " ('youre', 84),\n",
       " ('everyone', 84),\n",
       " ('find', 83),\n",
       " ('blackberry', 83),\n",
       " ('cant', 82),\n",
       " ('fun', 82),\n",
       " ('digital', 82),\n",
       " ('may', 81),\n",
       " ('phone', 81),\n",
       " ('thing', 81),\n",
       " ('back', 81),\n",
       " ('look', 81),\n",
       " ('looking', 79),\n",
       " ('nice', 79),\n",
       " ('��', 79),\n",
       " ('also', 78),\n",
       " ('web', 78),\n",
       " ('2s', 78),\n",
       " ('ever', 77),\n",
       " ('3', 76),\n",
       " ('many', 76),\n",
       " ('getting', 76),\n",
       " ('could', 76),\n",
       " ('away', 75),\n",
       " ('yes', 74),\n",
       " ('temp', 74),\n",
       " ('wait', 73),\n",
       " ('wins', 73),\n",
       " ('designing', 72),\n",
       " ('tv', 72),\n",
       " ('long', 71),\n",
       " ('quotgoogle', 71),\n",
       " ('includes', 69),\n",
       " ('uberguide', 69),\n",
       " ('bing', 69),\n",
       " ('giving', 68),\n",
       " ('interesting', 68),\n",
       " ('ill', 67),\n",
       " ('already', 67),\n",
       " ('much', 67),\n",
       " ('night', 65),\n",
       " ('every', 65),\n",
       " ('please', 65),\n",
       " ('made', 65),\n",
       " ('others', 65),\n",
       " ('looks', 64),\n",
       " ('tomorrow', 64),\n",
       " ('ready', 64),\n",
       " ('5', 64),\n",
       " ('take', 63),\n",
       " ('work', 63),\n",
       " ('sure', 62),\n",
       " ('someone', 62),\n",
       " ('sell', 62),\n",
       " ('fast', 62),\n",
       " ('interactive', 62),\n",
       " ('friends', 62),\n",
       " ('available', 62),\n",
       " ('live', 62),\n",
       " ('smart', 61),\n",
       " ('oh', 60),\n",
       " ('platform', 60),\n",
       " ('10', 60),\n",
       " ('details', 60),\n",
       " ('itunes', 59),\n",
       " ('product', 59),\n",
       " ('else', 58),\n",
       " ('tweet', 58),\n",
       " ('yet', 58),\n",
       " ('cc', 58),\n",
       " ('keep', 58),\n",
       " ('better', 58),\n",
       " ('years', 57),\n",
       " ('gets', 57),\n",
       " ('wow', 57),\n",
       " ('relief', 57),\n",
       " ('two', 57),\n",
       " ('tech', 56),\n",
       " ('street', 56),\n",
       " ('theres', 56),\n",
       " ('action', 56),\n",
       " ('saw', 56),\n",
       " ('updates', 55),\n",
       " ('join', 55),\n",
       " ('facebook', 55),\n",
       " ('battery', 55),\n",
       " ('meet', 54),\n",
       " ('na', 54),\n",
       " ('thats', 54),\n",
       " ('white', 54),\n",
       " ('vs', 54),\n",
       " ('technology', 54),\n",
       " ('far', 53),\n",
       " ('laptop', 53),\n",
       " ('seen', 53),\n",
       " ('quotapple', 53),\n",
       " ('japan', 53),\n",
       " ('begins', 53),\n",
       " ('hotpot', 53),\n",
       " ('gt', 52),\n",
       " ('stop', 52),\n",
       " ('marketing', 52),\n",
       " ('heading', 52),\n",
       " ('room', 52),\n",
       " ('2011', 52),\n",
       " ('hope', 51),\n",
       " ('days', 51),\n",
       " ('comes', 51),\n",
       " ('media', 51),\n",
       " ('help', 51),\n",
       " ('connect', 51),\n",
       " ('service', 51),\n",
       " ('another', 51),\n",
       " ('say', 51),\n",
       " ('might', 51),\n",
       " ('20', 51),\n",
       " ('sampler', 51),\n",
       " ('sponsored', 50),\n",
       " ('post', 50),\n",
       " ('person', 50),\n",
       " ('waiting', 50),\n",
       " ('let', 50),\n",
       " ('working', 50),\n",
       " ('geek', 50),\n",
       " ('taking', 50),\n",
       " ('codes', 50),\n",
       " ('lot', 49),\n",
       " ('guide', 49),\n",
       " ('navigation', 49),\n",
       " ('40', 49),\n",
       " ('physical', 48),\n",
       " ('art', 48),\n",
       " ('left', 48),\n",
       " ('valid', 48),\n",
       " ('theyre', 47),\n",
       " ('worlds', 47),\n",
       " ('block', 47),\n",
       " ('quotthe', 47),\n",
       " ('data', 47),\n",
       " ('gave', 47),\n",
       " ('money', 47),\n",
       " ('content', 47),\n",
       " ('event', 46),\n",
       " ('follow', 46),\n",
       " ('give', 46),\n",
       " ('release', 46),\n",
       " ('presentation', 46),\n",
       " ('convention', 45),\n",
       " ('demo', 45),\n",
       " ('photos', 45),\n",
       " ('making', 45),\n",
       " ('happy', 44),\n",
       " ('map', 44),\n",
       " ('sold', 44),\n",
       " ('core', 44),\n",
       " ('charger', 44),\n",
       " ('building', 44),\n",
       " ('excited', 43),\n",
       " ('parties', 43),\n",
       " ('foursquare', 43),\n",
       " ('weekend', 43),\n",
       " ('events', 43),\n",
       " ('trying', 43),\n",
       " ('saving', 43),\n",
       " ('morning', 42),\n",
       " ('quot', 42),\n",
       " ('flipboard', 42),\n",
       " ('info', 42),\n",
       " ('used', 42),\n",
       " ('share', 42),\n",
       " ('doesnt', 42),\n",
       " ('plenty', 42),\n",
       " ('front', 42),\n",
       " ('life', 42),\n",
       " ('company', 41),\n",
       " ('wifi', 41),\n",
       " ('world', 41),\n",
       " ('ones', 41),\n",
       " ('attendees', 41),\n",
       " ('guys', 41),\n",
       " ('must', 40),\n",
       " ('takes', 40),\n",
       " ('pic', 40),\n",
       " ('traffic', 40),\n",
       " ('makes', 40),\n",
       " ('amazing', 40),\n",
       " ('center', 40),\n",
       " ('hours', 40),\n",
       " ('texas', 40),\n",
       " ('outside', 40),\n",
       " ('things', 40),\n",
       " ('tell', 40),\n",
       " ('idea', 39),\n",
       " ('talking', 39),\n",
       " ('wonder', 39),\n",
       " ('cont', 39),\n",
       " ('business', 39),\n",
       " ('playing', 39),\n",
       " ('screen', 39),\n",
       " ('something', 39),\n",
       " ('quoti', 39),\n",
       " ('without', 39),\n",
       " ('showing', 39),\n",
       " ('website', 38),\n",
       " ('launches', 38),\n",
       " ('1st', 38),\n",
       " ('whats', 38),\n",
       " ('million', 38),\n",
       " ('opens', 38),\n",
       " ('wish', 38),\n",
       " ('found', 38),\n",
       " ('instead', 38),\n",
       " ('bring', 38),\n",
       " ('featured', 38),\n",
       " ('hootsuite', 37),\n",
       " ('power', 37),\n",
       " ('food', 37),\n",
       " ('lines', 37),\n",
       " ('didnt', 37),\n",
       " ('lol', 37),\n",
       " ('watch', 37),\n",
       " ('hear', 37),\n",
       " ('interfaces', 37),\n",
       " ('full', 37),\n",
       " ('group', 36),\n",
       " ('home', 36),\n",
       " ('play', 36),\n",
       " ('probably', 36),\n",
       " ('version', 36),\n",
       " ('blog', 36),\n",
       " ('stores', 36),\n",
       " ('place', 36),\n",
       " ('preview', 36),\n",
       " ('bad', 36),\n",
       " ('artists', 36),\n",
       " ('able', 36),\n",
       " ('usage', 36),\n",
       " ('qampa', 36),\n",
       " ('gowalla', 35),\n",
       " ('needs', 35),\n",
       " ('launched', 35),\n",
       " ('schools', 35),\n",
       " ('phones', 35),\n",
       " ('pretty', 35),\n",
       " ('march', 35),\n",
       " ('meetup', 35),\n",
       " ('hot', 35),\n",
       " ('stuff', 34),\n",
       " ('photo', 34),\n",
       " ('bought', 34),\n",
       " ('listening', 34),\n",
       " ('folks', 34),\n",
       " ('giveaway', 34),\n",
       " ('huge', 34),\n",
       " ('access', 34),\n",
       " ('daily', 34),\n",
       " ('never', 34),\n",
       " ('feel', 34),\n",
       " ('checking', 34),\n",
       " ('forward', 33),\n",
       " ('rock', 33),\n",
       " ('update', 33),\n",
       " ('ppl', 33),\n",
       " ('site', 33),\n",
       " ('industry', 33),\n",
       " ('buzz', 33),\n",
       " ('tweets', 33),\n",
       " ('hand', 33),\n",
       " ('quotcirclesquot', 33),\n",
       " ('true', 33),\n",
       " ('seems', 33),\n",
       " ('user', 33),\n",
       " ('sign', 33),\n",
       " ('ranking', 33),\n",
       " ('conference', 32),\n",
       " ('starting', 32),\n",
       " ('hour', 32),\n",
       " ('thank', 32),\n",
       " ('nerds', 32),\n",
       " ('brain', 32),\n",
       " ('experts', 32),\n",
       " ('maybe', 32),\n",
       " ('man', 32),\n",
       " ('lets', 32),\n",
       " ('ballroom', 32),\n",
       " ('experience', 32),\n",
       " ('built', 32),\n",
       " ('sets', 32),\n",
       " ('hilton', 32),\n",
       " ('code', 32),\n",
       " ('places', 32),\n",
       " ('lots', 32),\n",
       " ('walking', 32),\n",
       " ('conferencesquot', 32),\n",
       " ('youll', 31),\n",
       " ('put', 31),\n",
       " ('attending', 31),\n",
       " ('heres', 31),\n",
       " ('book', 31),\n",
       " ('read', 31),\n",
       " ('south', 31),\n",
       " ('everything', 31),\n",
       " ('diller', 31),\n",
       " ('lost', 31),\n",
       " ('trade', 31),\n",
       " ('brilliant', 31),\n",
       " ('80s', 31),\n",
       " ('said', 31),\n",
       " ('festival', 30),\n",
       " ('late', 30),\n",
       " ('old', 30),\n",
       " ('selling', 30),\n",
       " ('ios', 30),\n",
       " ('real', 30),\n",
       " ('setting', 30),\n",
       " ('friday', 30),\n",
       " ('updated', 30),\n",
       " ('official', 30),\n",
       " ('head', 30),\n",
       " ('developers', 30),\n",
       " ('view', 30),\n",
       " ('3g', 29),\n",
       " ('seeing', 29),\n",
       " ('local', 29),\n",
       " ('times', 29),\n",
       " ('pics', 29),\n",
       " ('macbook', 29),\n",
       " ('microsoft', 29),\n",
       " ('since', 29),\n",
       " ('soon', 29),\n",
       " ('call', 29),\n",
       " ('dev', 29),\n",
       " ('including', 29),\n",
       " ('cnn', 29),\n",
       " ('devices', 29),\n",
       " ('start', 28),\n",
       " ('locationbased', 28),\n",
       " ('wan', 28),\n",
       " ('headaches', 28),\n",
       " ('card', 28),\n",
       " ('security', 28),\n",
       " ('queue', 28),\n",
       " ('crowd', 28),\n",
       " ('scheduled', 28),\n",
       " ('rewards', 28),\n",
       " ('forget', 28),\n",
       " ('gon', 28),\n",
       " ('brand', 28),\n",
       " ('choice', 28),\n",
       " ('market', 28),\n",
       " ('top', 27),\n",
       " ('30', 27),\n",
       " ('chance', 27),\n",
       " ('town', 27),\n",
       " ('behind', 27),\n",
       " ('pm', 27),\n",
       " ('track', 27),\n",
       " ('lounge', 27),\n",
       " ('7', 27),\n",
       " ('dude', 27),\n",
       " ('route', 27),\n",
       " ('enter', 27),\n",
       " ('mom', 27),\n",
       " ('sale', 26),\n",
       " ('quotpopupquot', 26),\n",
       " ('speakquot', 26),\n",
       " ('belinsky', 26),\n",
       " ('mac', 26),\n",
       " ('turn', 26),\n",
       " ('developer', 26),\n",
       " ('keynote', 26),\n",
       " ('strategy', 26),\n",
       " ('etc', 26),\n",
       " ('crazy', 26),\n",
       " ('interrupt', 26),\n",
       " ('programming', 26),\n",
       " ('2quot', 26),\n",
       " ('living', 26),\n",
       " ('wont', 26),\n",
       " ('analytics', 26),\n",
       " ('bar', 26),\n",
       " ('camera', 26),\n",
       " ('tweeting', 25),\n",
       " ('started', 25),\n",
       " ('based', 25),\n",
       " ('space', 25),\n",
       " ('atampt', 25),\n",
       " ('150', 25),\n",
       " ('juice', 25),\n",
       " ('genius', 25),\n",
       " ('anything', 25),\n",
       " ('packed', 25),\n",
       " ('special', 25),\n",
       " ('schedule', 25),\n",
       " ('everywhere', 25),\n",
       " ('tx', 25),\n",
       " ('regularly', 25),\n",
       " ('apparently', 25),\n",
       " ('nothing', 25),\n",
       " ('thought', 25),\n",
       " ('interview', 25),\n",
       " ('always', 25),\n",
       " ('congrats', 25),\n",
       " ('ask', 25),\n",
       " ('actually', 24),\n",
       " ('bands', 24),\n",
       " ('report', 24),\n",
       " ('yesterday', 24),\n",
       " ('opened', 24),\n",
       " ('funny', 24),\n",
       " ('beta', 24),\n",
       " ('sweet', 24),\n",
       " ('tablets', 24),\n",
       " ('catch', 24),\n",
       " ('sharing', 24),\n",
       " ('red', 24),\n",
       " ('almost', 24),\n",
       " ('information', 24),\n",
       " ('move', 24),\n",
       " ('boomers', 24),\n",
       " ('isnt', 23),\n",
       " ('stock', 23),\n",
       " ('haha', 23),\n",
       " ('register', 23),\n",
       " ('drink', 23),\n",
       " ('stay', 23),\n",
       " ('checkins', 23),\n",
       " ('grab', 23),\n",
       " ('barry', 23),\n",
       " ('went', 23),\n",
       " ('hit', 23),\n",
       " ('per', 23),\n",
       " ('wants', 23),\n",
       " ('standing', 23),\n",
       " ('results', 23),\n",
       " ('buying', 23),\n",
       " ('schemas', 23),\n",
       " ('attend', 23),\n",
       " ('stream', 23),\n",
       " ('maggie', 23),\n",
       " ('try', 23),\n",
       " ('v', 23),\n",
       " ('awards', 23),\n",
       " ('important', 23),\n",
       " ('geeks', 23),\n",
       " ('killer', 23),\n",
       " ('watching', 23),\n",
       " ('cases', 22),\n",
       " ('mark', 22),\n",
       " ('enjoy', 22),\n",
       " ('least', 22),\n",
       " ('city', 22),\n",
       " ('ipad2s', 22),\n",
       " ('took', 22),\n",
       " ('doodles', 22),\n",
       " ('early', 22),\n",
       " ('hall', 22),\n",
       " ('guess', 22),\n",
       " ('list', 22),\n",
       " ('notes', 22),\n",
       " ('r', 22),\n",
       " ('windows', 22),\n",
       " ('little', 22),\n",
       " ('charge', 22),\n",
       " ('id', 22),\n",
       " ('device', 22),\n",
       " ('sxswi', 22),\n",
       " ('finally', 22),\n",
       " ('internet', 22),\n",
       " ('end', 22),\n",
       " ('visit', 22),\n",
       " ('project', 22),\n",
       " ('holler', 21),\n",
       " ('gram', 21),\n",
       " ('done', 21),\n",
       " ('job', 21),\n",
       " ('tweetquot', 21),\n",
       " ('quotthink', 21),\n",
       " ('talks', 21),\n",
       " ('ok', 21),\n",
       " ('earthquake', 21),\n",
       " ('minutes', 21),\n",
       " ('learn', 21),\n",
       " ('picture', 21),\n",
       " ('course', 21),\n",
       " ('saves', 21),\n",
       " ('verizon', 21),\n",
       " ('miss', 21),\n",
       " ('later', 21),\n",
       " ('tracks', 21),\n",
       " ('ride', 21),\n",
       " ('fb', 21),\n",
       " ('maes', 21),\n",
       " ('features', 21),\n",
       " ('searches', 21),\n",
       " ('page', 21),\n",
       " ('communication', 20),\n",
       " ('showcased', 20),\n",
       " ('plus', 20),\n",
       " ('checkin', 20),\n",
       " ('droid', 20),\n",
       " ('gsdampm', 20),\n",
       " ('point', 20),\n",
       " ('wondering', 20),\n",
       " ('pick', 20),\n",
       " ('pay', 20),\n",
       " ('audience', 20),\n",
       " ('picked', 20),\n",
       " ('dj', 20),\n",
       " ('stupid', 20),\n",
       " ('song', 20),\n",
       " ('heads', 20),\n",
       " ('goes', 20),\n",
       " ('sitting', 20),\n",
       " ('beyond', 20),\n",
       " ('exhibit', 20),\n",
       " ('league', 20),\n",
       " ('extraordinary', 20),\n",
       " ('drinks', 20),\n",
       " ('charging', 20),\n",
       " ('contextual', 20),\n",
       " ('privacy', 20),\n",
       " ('6', 20),\n",
       " ('fight', 20),\n",
       " ('gtgt', 20),\n",
       " ('pack', 20),\n",
       " ('mike', 20),\n",
       " ('online', 20),\n",
       " ('tim', 19),\n",
       " ('enjoying', 19),\n",
       " ('discovery', 19),\n",
       " ('month', 19),\n",
       " ('ur', 19),\n",
       " ('rumored', 19),\n",
       " ('development', 19),\n",
       " ('southwest', 19),\n",
       " ('ux', 19),\n",
       " ('america', 19),\n",
       " ('havent', 19),\n",
       " ('enough', 19),\n",
       " ('companies', 19),\n",
       " ('shot', 19),\n",
       " ('hello', 19),\n",
       " ('across', 19),\n",
       " ('acc', 19),\n",
       " ('send', 19),\n",
       " ('thinks', 19),\n",
       " ('startups', 19),\n",
       " ('interface', 19),\n",
       " ('air', 19),\n",
       " ('rankings', 19),\n",
       " ('latest', 19),\n",
       " ('magazines', 19),\n",
       " ('released', 19),\n",
       " ('consider', 19),\n",
       " ('hard', 19),\n",
       " ('tip', 19),\n",
       " ('asked', 19),\n",
       " ('useful', 19),\n",
       " ('means', 18),\n",
       " ('ta', 18),\n",
       " ('easy', 18),\n",
       " ('worth', 18),\n",
       " ('except', 18),\n",
       " ('miles', 18),\n",
       " ('retail', 18),\n",
       " ('5pm', 18),\n",
       " ('missed', 18),\n",
       " ('rumors', 18),\n",
       " ('control', 18),\n",
       " ('house', 18),\n",
       " ('bringing', 18),\n",
       " ('question', 18),\n",
       " ('netflix', 18),\n",
       " ('bigger', 18),\n",
       " ('employees', 18),\n",
       " ('create', 18),\n",
       " ('works', 18),\n",
       " ('story', 18),\n",
       " ('speech', 18),\n",
       " ('whos', 18),\n",
       " ('feature', 18),\n",
       " ('calendar', 17),\n",
       " ('added', 17),\n",
       " ('god', 17),\n",
       " ('lt', 17),\n",
       " ('kawasaki', 17),\n",
       " ('value', 17),\n",
       " ('name', 17),\n",
       " ('saved', 17),\n",
       " ('together', 17),\n",
       " ('part', 17),\n",
       " ('film', 17),\n",
       " ('aclu', 17),\n",
       " ('spent', 17),\n",
       " ('blocks', 17),\n",
       " ('less', 17),\n",
       " ('believe', 17),\n",
       " ('plan', 17),\n",
       " ('holding', 17),\n",
       " ('15', 17),\n",
       " ('near', 17),\n",
       " ('surprise', 17),\n",
       " ('run', 17),\n",
       " ('test', 17),\n",
       " ('deal', 17),\n",
       " ('11', 17),\n",
       " ('offers', 17),\n",
       " ('anywhere', 17),\n",
       " ('past', 17),\n",
       " ('source', 17),\n",
       " ('super', 17),\n",
       " ('b', 17),\n",
       " ('nfc', 17),\n",
       " ('bag', 17),\n",
       " ('reality', 17),\n",
       " ('bet', 17),\n",
       " ('googlebing', 17),\n",
       " ('browser', 17),\n",
       " ('seo', 17),\n",
       " ('favorite', 17),\n",
       " ('reading', 17),\n",
       " ('hands', 17),\n",
       " ('trip', 17),\n",
       " ('fan', 16),\n",
       " ('review', 16),\n",
       " ('friend', 16),\n",
       " ('blogger', 16),\n",
       " ('hipsters', 16),\n",
       " ('cards', 16),\n",
       " ('3d', 16),\n",
       " ('band', 16),\n",
       " ('shows', 16),\n",
       " ('sales', 16),\n",
       " ('improve', 16),\n",
       " ('giant', 16),\n",
       " ('1pm', 16),\n",
       " ('engine', 16),\n",
       " ('437', 16),\n",
       " ('saying', 16),\n",
       " ('tattoo', 16),\n",
       " ('totally', 16),\n",
       " ('light', 16),\n",
       " ('afford', 16),\n",
       " ('therapy', 16),\n",
       " ('owners', 16),\n",
       " ('jealous', 16),\n",
       " ('peeps', 16),\n",
       " ('cab', 16),\n",
       " ('whole', 16),\n",
       " ('html5', 16),\n",
       " ('40075959p', 16),\n",
       " ('floor', 16),\n",
       " ('st', 16),\n",
       " ('drive', 16),\n",
       " ('fascist', 16),\n",
       " ('lonely', 16),\n",
       " ('recommendations', 16),\n",
       " ('dead', 15),\n",
       " ('running', 15),\n",
       " ('til', 15),\n",
       " ('leave', 15),\n",
       " ('ha', 15),\n",
       " ('mashable', 15),\n",
       " ('publishing', 15),\n",
       " ('album', 15),\n",
       " ('fans', 15),\n",
       " ('earth', 15),\n",
       " ('support', 15),\n",
       " ('headed', 15),\n",
       " ('tools', 15),\n",
       " ('credit', 15),\n",
       " ('biggest', 15),\n",
       " ('came', 15),\n",
       " ('chrome', 15),\n",
       " ('ago', 15),\n",
       " ('hi', 15),\n",
       " ('plane', 15),\n",
       " ('badge', 15),\n",
       " ('airport', 15),\n",
       " ('yeah', 15),\n",
       " ('success', 15),\n",
       " ('socialtypequot', 15),\n",
       " ('hackers', 15),\n",
       " ('speak', 15),\n",
       " ('secret', 15),\n",
       " ('vip', 15),\n",
       " ('plans', 15),\n",
       " ('fam', 15),\n",
       " ('small', 15),\n",
       " ('tool', 15),\n",
       " ('interested', 15),\n",
       " ('learning', 15),\n",
       " ('augmented', 15),\n",
       " ('120035959p', 15),\n",
       " ('loving', 15),\n",
       " ('vp', 15),\n",
       " ('tried', 15),\n",
       " ('original', 15),\n",
       " ('car', 15),\n",
       " ('sounds', 15),\n",
       " ('flight', 15),\n",
       " ('killing', 15),\n",
       " ('dear', 15),\n",
       " ('double', 15),\n",
       " ('yall', 15),\n",
       " ('thinking', 15),\n",
       " ('coffee', 15),\n",
       " ('chris', 15),\n",
       " ('article', 15),\n",
       " ('tyson', 15),\n",
       " ('push', 14),\n",
       " ('knows', 14),\n",
       " ('agree', 14),\n",
       " ('ice', 14),\n",
       " ('cream', 14),\n",
       " ('weird', 14),\n",
       " ('mention', 14),\n",
       " ('change', 14),\n",
       " ('ad', 14),\n",
       " ('lunch', 14),\n",
       " ('model', 14),\n",
       " ('2nd', 14),\n",
       " ('longer', 14),\n",
       " ('busy', 14),\n",
       " ('winning', 14),\n",
       " ('salon', 14),\n",
       " ('announces', 14),\n",
       " ('iphones', 14),\n",
       " ('chat', 14),\n",
       " ('circle', 14),\n",
       " ('arrived', 14),\n",
       " ('1230pm', 14),\n",
       " ('100', 14),\n",
       " ('met', 14),\n",
       " ('quotgroupon', 14),\n",
       " ('leaving', 14),\n",
       " ('customer', 14),\n",
       " ('flash', 14),\n",
       " ('330', 14),\n",
       " ('bit', 14),\n",
       " ('word', 14),\n",
       " ('walked', 14),\n",
       " ('glad', 14),\n",
       " ('meeting', 14),\n",
       " ('x', 14),\n",
       " ('tablet', 14),\n",
       " ('different', 14),\n",
       " ('yep', 14),\n",
       " ('12', 14),\n",
       " ('stories', 14),\n",
       " ('exclusive', 14),\n",
       " ('brought', 14),\n",
       " ('welcome', 14),\n",
       " ('9', 14),\n",
       " ('mistakes', 14),\n",
       " ('pearl', 14),\n",
       " ('confirmed', 14),\n",
       " ('blue', 14),\n",
       " ('omg', 14),\n",
       " ('247', 14),\n",
       " ('crowley', 14),\n",
       " ('abt', 14),\n",
       " ('uses', 14),\n",
       " ('sessions', 14),\n",
       " ('streaming', 14),\n",
       " ('upgrade', 13),\n",
       " ('wed', 13),\n",
       " ('putting', 13),\n",
       " ('midnight', 13),\n",
       " ('stand', 13),\n",
       " ('mine', 13),\n",
       " ('limited', 13),\n",
       " ('hotel', 13),\n",
       " ('70', 13),\n",
       " ('questions', 13),\n",
       " ('inside', 13),\n",
       " ('random', 13),\n",
       " ('missing', 13),\n",
       " ('sat', 13),\n",
       " ('hoot', 13),\n",
       " ('computer', 13),\n",
       " ('gen', 13),\n",
       " ('pro', 13),\n",
       " ('among', 13),\n",
       " ('pr', 13),\n",
       " ('hold', 13),\n",
       " ('mon', 13),\n",
       " ('explorer', 13),\n",
       " ('public', 13),\n",
       " ('valuable', 13),\n",
       " ('games', 13),\n",
       " ('connected', 13),\n",
       " ('officially', 13),\n",
       " ('pc', 13),\n",
       " ('spending', 13),\n",
       " ('doodle', 13),\n",
       " ('disaster', 13),\n",
       " ('n', 13),\n",
       " ('care', 13),\n",
       " ('though', 13),\n",
       " ('concert', 13),\n",
       " ('finding', 13),\n",
       " ('031211', 13),\n",
       " ('yr', 13),\n",
       " ('japanese', 13),\n",
       " ('expect', 13),\n",
       " ('mayers', 13),\n",
       " ('survive', 13),\n",
       " ('4g', 13),\n",
       " ('envy', 13),\n",
       " ('kind', 13),\n",
       " ('reason', 13),\n",
       " ('bings', 13),\n",
       " ('appears', 13),\n",
       " ('heart', 13),\n",
       " ('josh', 13),\n",
       " ('passes', 13),\n",
       " ('likely', 12),\n",
       " ('corner', 12),\n",
       " ('downloaded', 12),\n",
       " ('concept', 12),\n",
       " ('alarm', 12),\n",
       " ('print', 12),\n",
       " ('you�۪re', 12),\n",
       " ('7pm', 12),\n",
       " ('services', 12),\n",
       " ('contact', 12),\n",
       " ('created', 12),\n",
       " ('conf', 12),\n",
       " ('mind', 12),\n",
       " ('shiny', 12),\n",
       " ('thoughts', 12),\n",
       " ('matter', 12),\n",
       " ('focus', 12),\n",
       " ('64gig', 12),\n",
       " ('50', 12),\n",
       " ('extra', 12),\n",
       " ('issue', 12),\n",
       " ('minute', 12),\n",
       " ('announcing', 12),\n",
       " ('makeshift', 12),\n",
       " ('shit', 12),\n",
       " ('samsung', 12),\n",
       " ('celebrate', 12),\n",
       " ('nexus', 12),\n",
       " ('networking', 12),\n",
       " ('black', 12),\n",
       " ('dashboard', 12),\n",
       " ('present', 12),\n",
       " ('lose', 12),\n",
       " ('hes', 12),\n",
       " ('crew', 12),\n",
       " ('lucky', 12),\n",
       " ('button', 12),\n",
       " ('disc', 12),\n",
       " ('speakeasy', 12),\n",
       " ('coverage', 12),\n",
       " ('gearing', 12),\n",
       " ('ipod', 12),\n",
       " ('text', 12),\n",
       " ('directions', 12),\n",
       " ('half', 12),\n",
       " ('donating', 12),\n",
       " ('staring', 12),\n",
       " ('hanging', 12),\n",
       " ('podcast', 12),\n",
       " ('dancing', 12),\n",
       " ('announced', 12),\n",
       " ('theyll', 12),\n",
       " ('quotipad', 12),\n",
       " ('friendly', 12),\n",
       " ('docs', 12),\n",
       " ('email', 12),\n",
       " ('spot', 12),\n",
       " ('cross', 12),\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = word_counts(df_clean.text)\n",
    "wordcounts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_stem W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=df_stem.text, size=100, window=3, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(577074, 866780)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences=df_stem.text, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marketplac', 0.8339308500289917),\n",
       " ('android', 0.822823703289032),\n",
       " ('dl', 0.8225846290588379),\n",
       " ('droid', 0.8183708190917969),\n",
       " ('ride', 0.815941572189331),\n",
       " ('wa', 0.8087526559829712),\n",
       " ('io', 0.8081423044204712),\n",
       " ('blackberri', 0.7942501902580261),\n",
       " ('keep', 0.7869850397109985),\n",
       " ('also', 0.7834244966506958)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('iphon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36800176"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('ipad', 'iphon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean_df W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clean = Word2Vec(sentences=df_clean.text, size=100, window=3, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(562842, 866780)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clean.train(sentences=df_clean.text, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Vocab at 0x225d461be48>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clean.wv.vocab['3g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('android', 0.889495849609375),\n",
       " ('market', 0.884195864200592),\n",
       " ('marketplace', 0.8746926784515381),\n",
       " ('ios', 0.8510479927062988),\n",
       " ('working', 0.8361167311668396),\n",
       " ('also', 0.8299484252929688),\n",
       " ('updates', 0.8274936079978943),\n",
       " ('gram', 0.8266642689704895),\n",
       " ('song', 0.8248703479766846),\n",
       " ('development', 0.8247174024581909)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clean.wv.most_similar('iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('youll', 0.962291955947876),\n",
       " ('create', 0.948000967502594),\n",
       " ('you�۪ll', 0.9437388181686401),\n",
       " ('quotif', 0.9394602179527283),\n",
       " ('follow', 0.9375459551811218),\n",
       " ('quotipad', 0.9373168349266052),\n",
       " ('photo', 0.9371738433837891),\n",
       " ('user', 0.9368828535079956),\n",
       " ('ready', 0.9355154633522034),\n",
       " ('borrow', 0.9337400794029236)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clean.wv.most_similar('tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6959976"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clean.wv.similarity('rock', 'iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3466813"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clean.wv.similarity('ipad', 'iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73951"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clean.wv.similarity('laptop', 'iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54867953"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clean.wv.similarity('fire', 'iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         Negative emotion\n",
       "1                         Positive emotion\n",
       "2                         Positive emotion\n",
       "3                         Negative emotion\n",
       "4                         Positive emotion\n",
       "                       ...                \n",
       "8716                      Positive emotion\n",
       "8717    No emotion toward brand or product\n",
       "8718    No emotion toward brand or product\n",
       "8719    No emotion toward brand or product\n",
       "8720    No emotion toward brand or product\n",
       "Name: emotion, Length: 8720, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Abhineet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarizer label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec first: vocab size, embeddings\n",
    "#    Would require an LSTM\n",
    "# Outputs of embedding:\n",
    "\n",
    "\n",
    "# Final layer with softmax: Ternary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams / digrams as features for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler NN would be putting word counts into embedding layer as first layer\n",
    "# Dense layers with dropout up until output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe Things Are Looking Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcounts - Just gets all the amounts of times a word is used. \"Ten or more\" is a list with only the words used ten times plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts(text):\n",
    "    wordcount = Counter()\n",
    "    for i in text.values:\n",
    "        for x in i:\n",
    "            wordcount[x] += 1\n",
    "    return wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = word_counts(df_clean.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_or_more = [x for x, y in wordcounts.items() if y > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the word count drop any with <10 occurence\n",
    "# This becomes embedding layer's input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label binarizer - This turned happy, neutral, sad to 001, 010, 100. Or something like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order of 1's position it goes: Negative, neutral, positive\n",
    "y = lb.fit_transform(df_clean.emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer - This basically just made a list of every word and then gave it a unique ID? I don't get the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.fit_on_texts(ten_or_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = tk.texts_to_sequences(ten_or_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_test, y_train, y_test = train_test_split(df_clean.text, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed vect text thing with Abhineet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_text(text):\n",
    "    text = tensorflow.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_layer = TextVectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.map(vect_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.add(Embedding(input_dim=1082, output_dim=100))\n",
    "model_new.add(Dense(30, activation=\"relu\"))\n",
    "model_new.add(Dense(3, activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
